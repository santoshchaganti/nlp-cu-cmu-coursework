{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ead33c3-be2a-4cbb-87b6-10f7f68ff204",
   "metadata": {},
   "source": [
    "# Assigment 4: Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52f7d690-8d5a-49a8-a6ca-89712a01ce65",
   "metadata": {},
   "source": [
    "In this assignment, you will be implementing a simplified Transformer from scratch, and training it to count letters used in the sequence.\n",
    "\n",
    "You are expressively not-allowed to use any off-the-shelf implementations of attention mechanisms, such as `nn.TransformerEncoder`, `nn.TransformerDecoder`, etc. \n",
    "\n",
    "The assignment can be completed using only the functions we have imported and Tensor manipulations (`squeeze`, `unsqueeze`, `view`, `split`, `chunk`, `concat`). \n",
    "\n",
    "If you desire to use others, feel free to ask on Piazza or at Office Hours.\n",
    "\n",
    "![trn.svg](trn.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac6933e-cfed-4447-97d9-6f8bc7cc459e",
   "metadata": {},
   "source": [
    "## 1) Multi-head Attention\n",
    "\n",
    "In this section, you will be implementing an Encoder transformer block with no LayerNorm that logs the attention maps. \n",
    "\n",
    "Tips:\n",
    "- We reccomend you start with a single head, and make sure to check the dimensions of your Tensors with `.shape` after each calculation. \n",
    "- Make sure to store a deep copy of your residuals, rather than references.\n",
    "- The attention map(s) are the $softmax(\\frac{Q K^T}{\\sqrt{d_k}})$ term(s).\n",
    "    - We recomend you cache this as a layer-property rather than include it during inference, as the latter strategies complicates transformers with more than one layer.\n",
    " \n",
    "![f.svg](f.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac4516-ecd2-4911-9657-c10bae9ded32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install tqdm\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea199e-1198-46cf-9f72-465ad7090225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Linear, ReLU\n",
    "from torch.nn.functional import softmax\n",
    "from torch import matmul, zeros, concat, cat\n",
    "from math import sqrt\n",
    "\n",
    "class TransformerLayer(Module):\n",
    "    def __init__(self, d_model, d_internal, d_ff, n_heads, max_seq_len=20):\n",
    "        \"\"\"\n",
    "        :param d_model: The dimension of the inputs and outputs of the layer \n",
    "            (note that the inputs and outputs have to be the same size for the residual connection to work)\n",
    "        :param d_internal: The \"internal\" dimension used in the self-attention computation. \n",
    "            Your keys and queries should both be of this length per head.\n",
    "        :param d_ff: The \"feed forward\" dimension used in the feed-foward computation. \n",
    "            You should have a linear transformation from d_model to d_ff, and then following an activation, d_ff to d_model.\n",
    "        :param n_heads: The number of attention heads used. \n",
    "            Keep this number low for efficiency.\n",
    "        :param max_seq_len: The max sequence length. \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_internal = d_internal\n",
    "        self.n_heads = n_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.last_map = zeros(n_heads,max_seq_len,max_seq_len)\n",
    "\n",
    "        # TODO 1.1) Define the QKV linear transformations\n",
    "        self.q_proj = ...\n",
    "        self.k_proj = ...\n",
    "        self.v_proj = ...\n",
    "\n",
    "        # TODO 1.2) Define the multi-head attention linear transformations\n",
    "        self.mha_proj = ...\n",
    "                \n",
    "        # TODO 1.3) Define the feedforward \n",
    "        self.ff1 = ...\n",
    "        self.activation = ...\n",
    "        self.ff2 = ....\n",
    "\n",
    "    \n",
    "    def forward(self, input_vecs):\n",
    "        \"\"\"\n",
    "        :param input_vecs: an input tensor of shape [seq len, d_model]\n",
    "        :return: a tensor of shape [seq len, d_model] representing the log probabilities of each position in the input     \n",
    "        \"\"\"\n",
    "\n",
    "        q,k,v = input_vecs,input_vecs,input_vecs\n",
    "\n",
    "        # TODO 1.5) Calculate MHA up to the attention map\n",
    "\n",
    "        # TODO 1.6) Cache latest attention map \n",
    "\n",
    "        # TODO 1.7) Complete MHA\n",
    "\n",
    "        # TODO 1.8) Add residual\n",
    "\n",
    "        # TODO 1.9) Apply feed forward and add second residual\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b2b52-27b5-4331-8404-c98a697bf014",
   "metadata": {},
   "source": [
    "#### Deliverable - Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c6a40-2f73-4a62-841c-7885abb7b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch import manual_seed,randn\n",
    "from numpy import round as npround\n",
    "\n",
    "random.seed(42)\n",
    "manual_seed(42)\n",
    "\n",
    "# Single-head test case\n",
    "input_tensor = randn(5,8)\n",
    "\n",
    "sh_attn = TransformerLayer(d_model=8,d_internal=32,d_ff=64, n_heads=1,max_seq_len=5)\n",
    "print('Single-head output')\n",
    "print(npround(sh_attn(input_tensor).detach().numpy(),4),'\\n')\n",
    "print('Single-head attention map')\n",
    "print(npround(sh_attn.last_map.detach().numpy(),4),'\\n')\n",
    "\n",
    "print('='*60,'\\n')\n",
    "\n",
    "# Mutlti-head test case\n",
    "sh_attn = TransformerLayer(d_model=8,d_internal=16,d_ff=128, n_heads=2,max_seq_len=5)\n",
    "\n",
    "print('Multi-head output')\n",
    "print(npround(sh_attn(input_tensor).detach().numpy(),4),'\\n')\n",
    "print('Multi-head attention maps')\n",
    "print(npround(sh_attn.last_map.detach().numpy(),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c4398-d4fd-4140-87ff-50305e1862fe",
   "metadata": {},
   "source": [
    "## 2) Embeddings and Positional Encoding\n",
    "\n",
    "Next we will be creating the Transformer model base.\n",
    "\n",
    "Below you will need to implement the embeddings, as well as a forward function.\n",
    "\n",
    "Each token's embedding should be the sum of two embeddings; \n",
    "1. the word embedding that embeds the vocabulary in a `d_model`-dim space.\n",
    "2. the absolute positional encoding that embeds each index in a `d_model`-dim space. Since all the inputs in this assignment will be of length `max_seq_len`, you can consider it a mapping from the `max_seq_len`-dim indices to `d_model`, and embed each index using a range.\n",
    "\n",
    "Lastly the forward should:\n",
    "1. Apply the embeddings\n",
    "2. Apply the `nn.Sequential` of the Encoder layers\n",
    "    - And after this, iterate through the `nn.Sequential to retrieve the cached maps for each layer\n",
    "3. Apply the simplified decoder layer\n",
    "4. Convert the outputs to log probabilities over the tokens, and return them along with the maps\n",
    "\n",
    "![pos.svg](pos.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4904b-fbb1-465e-9f9a-440f030dd4bd",
   "metadata": {},
   "source": [
    "<html></html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ae05d-fff9-452a-9118-941be296fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor, LongTensor, FloatTensor, log, concat, cat\n",
    "from torch.nn import Sequential, Embedding\n",
    "from torch.nn.functional import log_softmax\n",
    "\n",
    "class Transformer(Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, d_internal, num_classes, d_ff, num_layers, num_heads):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocabulary size of the embedding layer\n",
    "        :param max_seq_len: max sequence length that will be fed to the model; should be 20\n",
    "        :param d_model: see TransformerLayer\n",
    "        :param d_internal: see TransformerLayer\n",
    "        :param d_ff: see TransformerLayer\n",
    "        :param num_classes: number of classes predicted at the output layer; should be 3\n",
    "        :param num_layers: number of TransformerLayers to use; can be whatever you want\n",
    "        :param num_layers: number of heads per TransformerLayer; can be whatever you want\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layer = TransformerLayer(d_model=d_model, d_internal=d_internal,n_heads=num_heads,d_ff=d_ff,max_seq_len=max_seq_len)\n",
    "            layers.append(layer)\n",
    "        self.encoders = Sequential(*layers)\n",
    "        self.decoder = Linear(d_model, num_classes)\n",
    "\n",
    "        # TODO 2.1) Define word embedding\n",
    "\n",
    "        # TODO 2.2) Define positional encoding\n",
    "\n",
    "\n",
    "    def forward(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: list of input indices\n",
    "        :return: A tuple of the softmax log probabilities (should be a 20x3 matrix) and a list of the attention\n",
    "        maps you use in your layers (can be variable length, but each should be a 20x20 matrix)\n",
    "        \"\"\"\n",
    "        # TODO 2.3) Apply word embedding and positional encoding\n",
    "\n",
    "        # TODO 2.4) Layer Forward Passes\n",
    "        \n",
    "        # TODO 2.5) Return the log-probabilites and cached maps for every layer and head\n",
    "        return (x, maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215441e5-6f9b-4baf-b575-f7c0d7cc41f5",
   "metadata": {},
   "source": [
    "#### Deliverable - Embedding\n",
    "\n",
    "Provide the embedding (word embedding plus positional encoding) for [1,1,2,2,3], \n",
    "using d_model=16, max_seq_len=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d9e0dd-bab4-49fe-a154-ac9d9be0c9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "807783b0-1fa3-44c1-a7b8-6c293c8eaa34",
   "metadata": {},
   "source": [
    "## 3) Dataset Processing\n",
    "\n",
    "The dataset for this transformer implementation will be letter-counting. \n",
    "\n",
    "Given a string, your model should predict how many times that character has occurred previously.\n",
    "To simplify the computation, this will be split into bins of n=0, n=1, and n>=2\n",
    "\n",
    "Specifically for we be considering two settings:\n",
    "\n",
    "1) Task 1 - counts of the letter *before* the current index\n",
    "2) Task 2 - counts of the letter *before* and *after* the current index\n",
    "\n",
    "We provide helper functions for formatting the tasks, your job is to write a function `indexer` that takes the 20 character strings and converts them to `LongTensor`s of IDs. \n",
    "\n",
    "\n",
    "Read from `lettercounting-train.txt`, `letttercounting-dev.txt`.\n",
    "For both the train and eval set, collect an array for the strings, the ID tensors, and the gold standard outputs for both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eab1aa-edf9-46de-8c88-70c602a710ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import LongTensor\n",
    "\n",
    "def task1(input: str) -> LongTensor:\n",
    "    output = np.zeros(len(input))\n",
    "    for i in range(0, len(input)):\n",
    "        output[i] = min(2, len([c for c in input[0:i] if c == input[i]]))\n",
    "    return LongTensor(output)\n",
    "\n",
    "def task2(input: str) -> LongTensor:\n",
    "    output = np.zeros(len(input))\n",
    "    for i in range(0, len(input)):\n",
    "        output[i] = min(2, len([c for c in input if c == input[i]]) - 1)\n",
    "    return LongTensor(output)\n",
    "\n",
    "def indexer(input: str) -> LongTensor:\n",
    "    # TODO 3.1) map string to character index LongTensor.\n",
    "    # Note that all the characters are either spaces or the lowercase alphabet\n",
    "    return LongTensor(...)    \n",
    "\n",
    "\n",
    "with open('lettercounting-train.txt','r',encoding='utf8') as train_file:\n",
    "    # TODO 3.2) Process train_file\n",
    "    pass\n",
    "    \n",
    "with open('lettercounting-dev.txt','r',encoding='utf8') as eval_file:\n",
    "    # TODO 3.3) Process eval_file\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572c193-b49d-469e-9995-a5c51bce32d7",
   "metadata": {},
   "source": [
    "#### Deliverable: Provide the ID tensor and task tensors for the string \"ed by rank and file\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff68f4-14fd-4907-ac2b-7b4108c2d006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a99635db-fcbc-4ab7-b54a-578311007cc3",
   "metadata": {},
   "source": [
    "## 4) Training Loop\n",
    "\n",
    "Now that we have both our model and our dataset, it is time to train and evaluate on our tasks.\n",
    "\n",
    "The following code blocks provide a helper function for visualizing your attention maps (so you can see it learning), as well as loss functions / optimizers / learning rate schedulers that may come in handy.\n",
    "\n",
    "To complete this section, you must provide us with models that achieve >90% eval accuracy on each task.\n",
    "If implemented properly, this can be done with 1 layer and 1 head/layer, however, we encourage you to experiment.\n",
    "\n",
    "\n",
    "\n",
    "In your solution, provide us with the following:\n",
    "- train losses logged at ~100-250 steps\n",
    "- eval losses logged every ~5-10 train loss updates\n",
    "- for each eval update, the accuracy and macro-F1 scores of the train/eval.\n",
    "    - this is best done accumulating arrays of the predictions and model outputs throughout the training loop, and then calls the to the scikit-learn metrics.\n",
    "\n",
    "You can return these in any form you would like (table, graph, etc), as long as they are immediately interpretable (for example, normalizing the train/eval loss for number of examples would be necessary).\n",
    "\n",
    "\n",
    "Additionally, describe any issues you had with selecting on optimal learning rate, loss function, etc.\n",
    "\n",
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f45d3-6951-4678-9b07-e7c76626daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note, if you are unable to store the outputs from plt.show(), configure the plot_maps function to use savefig()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_maps(maps,text):\n",
    "    maps = maps.detach().numpy()\n",
    "    num_layers = maps.shape[0]\n",
    "    num_heads = maps.shape[1]\n",
    "    fig, axs = plt.subplots(num_heads,num_layers,figsize=(num_layers*4,num_heads*4))\n",
    "\n",
    "    for head_index in range(num_heads):\n",
    "        for layer_index in range(num_layers):\n",
    "            if num_heads==1 and num_layers == 1:\n",
    "                ax = axs\n",
    "            elif num_layers==1:\n",
    "                ax = axs[head_index]\n",
    "            elif num_heads==1:\n",
    "                ax = axs[layer_index]\n",
    "            else:\n",
    "                ax = axs[head_index][layer_index]\n",
    "            attn_map = maps[layer_index][head_index]\n",
    "            ax.imshow(attn_map, cmap='hot', interpolation='nearest')\n",
    "            ax.set_xticks(np.arange(len(text)), labels=text)\n",
    "            ax.set_yticks(np.arange(len(text)), labels=text)\n",
    "            ax.xaxis.tick_top()\n",
    "            \n",
    "    # if not using juypter notebook \n",
    "    #plt.savefig(\"maps.png\")\n",
    "    # otherwise\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78220ab9-07bc-4059-8bc0-a5196fa193be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "from torch.nn import NLLLoss,BCELoss,CrossEntropyLoss\n",
    "# Optimizers\n",
    "from torch.optim import SGD, Adam, AdamW, Adadelta \n",
    "# Learning rate schedulers (optional)\n",
    "from torch.optim.lr_scheduler import LinearLR, StepLR, CosineAnnealingLR\n",
    "\n",
    "from torch import no_grad\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "model = ...\n",
    "optimizer = ...\n",
    "num_epochs = ...\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    model.train()\n",
    "    # Train loop\n",
    "    # Log losses, predictions/gold standards for accuracy/F1\n",
    "\n",
    "    model.eval()\n",
    "    with no_grad():\n",
    "        # Eval loop\n",
    "            # Log losses, predictions/gold standards for accuracy/F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828debd5-e565-4c7c-aa2f-f96c5736d30a",
   "metadata": {},
   "source": [
    "#### Deliverable: the chart or graph of the training/eval for both models. Make sure to label the losses/acc/f1 as train/eval, and normalize the losses by the number of data points they are derived from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146593e-2fe4-4709-8ef3-e5c7a0d4f8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e145f51-085e-4752-82a0-171ee8786a83",
   "metadata": {},
   "source": [
    "## 5) Visualization\n",
    "Lastly we will examine the computation performed by the model(s) you've trained using its attention maps.\n",
    "\n",
    "The following are 1-layer, 1-head models trained on Task 1 (the before task), and Task 2 (the before/after task) respectively. They both achieve accuracy ~95%.\n",
    "<div class=\"row\">\n",
    "<img src=\"data:image/png;base64,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\">\n",
    "<img src=\"data:image/png;base64,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\"</div>\n",
    "\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e74aa2-a699-43ed-b986-44c11dfdc0ac",
   "metadata": {},
   "source": [
    "#### Deliverable 5.1) Hypothesize on how these attention maps reflect the computation done by their associated models. Specifically address how each handles the tokens ['e', 'a', 'n', 'd', ' '], how they are similar, and how/why they may be different.\n",
    "\n",
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e7db2-90c4-45f7-b6f7-cc8f159ab9cb",
   "metadata": {},
   "source": [
    "#### Deliverable 5.2) Train a multi-layer (>4), single-head model on one of the tasks. \n",
    "Achieve an eval accuracy of at least 80%, and plot the attention maps below on the string `\"ed by rank and file\"`.\n",
    "\n",
    "Hypothesize on how each layer contributes to the classification.\n",
    "\n",
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17286161-4edc-4d99-8079-4f9d4f0bf313",
   "metadata": {},
   "source": [
    "#### Deliverable 5.3) Train a multi-head model on one of the tasks.\n",
    "Achieve an eval accuracy of at least 80%, and plot the attention maps below on the string `\"ed by rank and file\"`.\n",
    "\n",
    "Hypothesize on how each head contributes to the classification.\n",
    "\n",
    "ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b7aa8-1900-456e-9471-a6e4b853621e",
   "metadata": {},
   "source": [
    "#### Acknowledgement\n",
    "\n",
    "This homework was adapted from CS388, taught by Greg Durrett at University of Texas Austin, and CS685, taught by Mohit Iyyer, then at University of Massachusetts Amherst.\n",
    "\n",
    "The visualizations take inspiration from a twitter post by Mark Riedl of Georgia Tech. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
